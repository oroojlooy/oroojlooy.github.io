<!DOCTYPE html>
<html>
	<head>
<style>
img {
    display: block;
    margin: 0 auto;
}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    inlineMath: [['$','$']]
  }
});
</script>



<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script>

<script type="text/javascript"
	src="http://www.maths.nottingham.ac.uk/personal/drw/LaTeXMathML.js">
</script>

<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<style>
  pre .latex { border:3px solid red ; }
</style>

		<title>Blog</title>
		<!-- link to main stylesheet -->
		<link rel="stylesheet" type="text/css" href="/css/main.css">		
	</head>
	<body>
		<nav>
    		<ul>
        		<li><a href="/">Home</a></li>
	        	<li><a href="/publications">Publications</a></li>
        		<li><a href="/cv">CV</a></li>
        		<li><a href="/blog">Blog</a></li>
    		</ul>
		</nav>
		<div class="container">
    		<div class="blurb">
				<font size="+2"> <p><em>  <b> Beer Game  </b> </em> </p> </font>
				<p> 	The newsvendor problem optimizes the inventory of a perishable good. Perishable goods are those that have a limited selling season; they include fresh produce, newspapers, airline tickets, and fashion goods. The newsvendor problem assumes that the company purchases the goods at the beginning of  a time period and sells them during the period. 
				At the end of the period, unsold goods must be discarded, incurring a {\em holding cost}.
				In addition, if it runs out of the goods in the middle of the period, it incurs a {\em shortage cost}, losing potential profit.
				Therefore, the company wants to choose the order quantity that minimizes the expected sum of the two costs described above. The problem dates back to 1888. See ???? \cite{Porteus08} for a history. <p>
				<!-- alt="The agents of beer game"  -->
				
				<p> The optimal order quantity for the newsvendor problem can be obtained by solving the following optimization problem:					
					<img src="http://mathurl.com/y7z7skg3.png">
				where $d$ is the random demand, $y$ is the order quantity, [$c_p$] and $c_h$ are the per-unit shortage and holding costs (respectively). In the presence of the demand distribution, the optimal order quantity can be obtained by solving the optimization problem \cite{snyder2011fundamentals}. In the classical version of the problem, the demand distribution (e.g., normal) and its parameters are either known or estimated using available data. </p> 

				<p>There are numerous extensions of this problem (see ???). One of the extension is <i> multi-dimensional features newsvendor problem</i>, in which the demand from each of the $n$ items comes with some additional information, e.g. weather conditions, day of the week, store location, etc.</p> 
<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\dot{x} & = \sigma(y-x) \\
\dot{y} & = \rho x - y - xz \\
\dot{z} & = -\beta z + xy
\end{aligned} %]]></script>

				<p>There are ??? main approaches for solving the multi-feature newsvendor problem with data features. 
				First, the classical approach obtains an estimate of the demand's probability distribution and then uses the classical optimization model to obtain the order quantities. This approach is called separated estimation and optimization (SEO).
				However, obtaining the probability distribution with any approximation procedure involves some level of errors, especially when there are insufficient historical data. 
				In addition, sometimes there is not enough data to approximate a probability distribution. 
				So, in each of these cases, the classical approach cannot be used.</p> 
				
				<p>Second, \cite{Thiele} proposes a data driven algorithm that directly optimizes the order quantities through finding Estimated Quantile (EQ).
				However, this model fails when there is volatility among the training data. 
				In addition, similar to the classical newsvendor algorithm, it only considers categorical features and cannot deal with continuous features, which are common in supply chain demand data.</p> 

				<p>Finally, machine learning and Deep Neural Network (DNN) models have been proposed to obtain a forecast of the demand, e.g. \cite{lu2014hybrid}.
				Most of these approaches set order quantities equal to the demand forecast. However, this ignores a core insight from the newsvendor model, namely, that order quantities should be larger (or, in rarer cases, smaller) than the demand forecast, with the actual quantity driven by the relative magnitudes of the shortage and holding costs. 
				To address this drawback, \cite{vahn_News} proposes a Linear Machine Learning (LML) model, which considers the cost coefficients to obtain order quantities.
				However, this model is too slow for a large number of instances, has poor quality when there are small number of observations for each combination of the features, and its learning is limited to the current training data. 
				Therefore, there is a need for an algorithm that works with both small and large amounts of featured-historical data and solve them in a reasonable amount of time.</p> 

				<p> <em>  <b> Our Approach  </b> </em> <p>
				<p> We develop a new approach based on Deep Neural Network (DNN) to solve the multi-product newsvendor problem with data features. 
				DNN is a branch of machine learning which aimed to build a model between inputs and outputs. 
				In order to minimize the closeness of the obtained values and the real outputs, a loss function, e.g. hinge, euclidean, etc. measures the closeness of them.
				The goal of DNN is providing a small loss value.
				To adapt DNN for the newsvendor problem, we propose two loss functions, considering the impact of the shortage and holding costs. 
				These loss functions <i>allow DNN to obtain the minimizer of the newsvendor cost function directly, rather than first estimating the demand distribution and then choosing an order quantity.</i>
				Thus, feeding the historical data into the corresponding DNN and running it gives the order quantities directly.
				Two proposed loss functions are based on the newsvendor cost function and we call them DNN-$\ell_1$ and DNN-$\ell_2$, according to their structures. </p>


				<p> An adjusted Deep Q-Network (DQN) model is used to solve to this problem. DQN is used to solve single agent games and extended to two agents zero-sum games. However, beer game is a cooperative non-zero-sum game. Using DQN directly results in a game that each agent minimizes its own cost not the whole cost of system. Thus, we cannot use it directly to solve the problem. We propose a feedback scheme to fix this issue and it enforces the agents to learn to minimize the whole cost of system. In this order we update the observed reward of each agent <i>i</i> and period <i>t</i> using:</p>
				<img src="http://mathurl.com/y8auqxt8.png">
				<p> in which we indeed penalize the agent with the summation of cost of other agents. Therefore, agents learn to minimize the total cost of system instead of just their own costs. <p> 


				<!-- alt="results"  -->
				<p> <em> <b> What does it Learn? </b> </em>  <p>
				<p> We present the results from a straightforward version of the game in which the DQN plays the role of the wholesaler and the other three players each follow a base-stock policy. (A base-stock policy means we always order to bring the inventory position&mdash;on-hand inventory minus backorders plus on-order inventory&mdash;equal to a fixed value, called the base-stock level. A base-stock policy is known to be optimal for the beer game, assuming all four players use it.) Each stage has a holding cost of <i>2 per item per period. The retailer has a shortage cost of <i>2 per item per period and the other stages do not have a shortage cost. The customer demand is either 0, 1, or 2 in each time period. (If these numbers seem small to you, think of them as cases or truckloads.) And, here is the way that the wholesaler learns:<p>
<div align="center">
					<iframe width="800" height="600" src="https://www.youtube.com/embed/gQa6iWGcGWY" frameborder="0" gesture="media" allowfullscreen></iframe>
</div>
				<p>Before the algorithm is trained, the wholesaler plays pretty badly. It stocks way too much inventory, as you can see in the figure below, and causes stockouts at the upstream stages. (Positive inventory is shown as brown boxes and positive numbers. Backorders are shown as red boxes and negative numbers.) <p> 
				<img src="before_training_100.jpg" alt="before training" width="680" height="382" align="middle"/> 
				
				<p> After 1000 of training episodes, it has learned that there is positive holding cost and it is better to keep its inventory level smaller. <p> 
				<img src="1000_training_100.jpg" alt="after 1000 training episodes" width="680" height="382" align="middle"/> 

				<p> After 2000 of training episodes, it has learned that there is no stock-out cost and it is better to keep the inventory its level negative. Also, as you see the whole network is more stable and the inventory levels are closer to zero than previous screenshots.<p> 
				<img src="2000_training_100.jpg" alt="after 2000 training episodes" width="680" height="382" align="middle"/> 

				<p> After 45000 of training episodes, it has learned almost the optimal policy, and plays in a way to not only keeps its inventory level and cost close to zero, but also plays in a way to minimize the total cost of the system. Apparently, whole network is more stable and the inventory levels are closer to zero than previous screenshots.<p> 
				<img src="45000_training_100.jpg" alt="after 45000 training episodes" width="680" height="382" align="middle"/> 

				<p> <em> <b> Numerical Results </b> </em> <p>
				Here we have the cumulative cost of four agents, when one of the agents follows DQN, and other agents playing base stock. In each figure, the upper subfigure provides the total cost, and the lower subfigure provides the normalized cost, in which the both BS policy and DQN costs are divided by BS policy's cost. It is worthy to mention that, for the proposed setting of game BS policy is optimal.
				As it you see, after few iterations all have learned to play close to optimal solution and continue it to end. The following figure represents the results of the case that DQN plays retailer:
				<img src="brain3_2_layer_lr000025_multPrd10_dnnUpCnt10000.png" alt="retailer follows DQN" width="680" height="536" align="middle"/> 
				in which there is about 8% of gap compared to the case that all agents follow base stock policy. 
				The following figure presents the results of the case that DQN plays wholesaler:
				<img src="brain4_3_layer_lr000025_multPrd10_dnnUpCnt10000.png" alt="wholesaler follows DQN" width="680" height="536" align="middle"/> 
				in which there is about 6% of gap compared to the case that all agents follow base stock policy. 
				The following figure provides the results of the case that DQN plays distributor:
				<img src="brain5_3_layer_lr000025_multPrd5_dnnUpCnt10000.png" alt="distributor follows DQN" width="680" height="536" align="middle"/> 
				in which there is about 7% of gap compared to the case that all agents follow base stock policy. 
				And finally, the following figure shows the results of the case that DQN plays manufacturer:
				<img src="brain6_3_layer_lr000025_multPrd5_dnnUpCnt10000.png" alt="manufacturer follows DQN" width="680" height="536" align="middle"/> 
				in which there is about 2% of gap compared to the case that all agents follow base stock policy. 


				Also, the following figure shows the details of the inventory level (<i>IL</i>), on-order quantity (<i>OO</i>), order quantity (<i>a</i>), order up to level (<i>OUTL</i>), and reward (<i>r</i>) for the retailer when the retailer is	played by the DQN. 
				The BS policy and DQN have similar <i>IL</i> and <i>OO</i> trends, and as a result their rewards are also very close.
				<img src="brain3-0-26-R.png" alt="the details of play when retailer follows DQN" width="1156" height="250" align="middle"/> 


				<p> <em> <b> Play with Irrational Players </b> </em> <p>
				The agents who follow the BS are rational players. In order to see how well DQN works when it plays with irrational players, we tested the case that a DQN agent plays with three agents following the Sterman formula; see the following figure, which shows the corresponding results when the DQN plays the distributor. The DQN learns very well to play with other irrational players to minimize the total cost of the system, such that on average the DQN agents obtained $40\%$ smaller cost than the case in which the three Sterman players play with one BS policy. We get same results on other agents. 
				<img src="brain9_3_layer_lr0001_multPrd5_dnnUpCnt5000.png" alt="distributor follows DQN" width="680" height="536" align="middle"/> 


				<p> <em> <b> Transfer Learning </b> </em> <p>
				This procedure works well; however, for each new set of <i>the number of actions, shortage and holding cost parameters</i> we need to train a new policy, since they uniquely define the DQN loss function, and this is a time consuming procedure. To address this issue, we train a base agent with given cost parameters and action space, then transfer the acquired knowledge to the new agents, so that the obtained policy easily can be generalized to other agents. 
				The following table shows the results of training new agents with different cost coefficients and different action spaces.
				As it is shown, the average gap is around 8.9\%, which shows the effectiveness of transfer learning approach. Additionally, the training times of each case are provided in the table. 
				In order to get the base results, we did hyper-parameter tuning which resulted in total of 13,354,951 seconds training. However, following the transfer learning approach we do not need any hyper-parameter tuning, and on average we spent 891,117 seconds for training, which is 15 times faster.
				<img src="tl_results.png" alt="results of transfer learning" width="797" height="252" align="middle"/> 


				<p> <em> <b> Conclusion </b> </em> <p>
					These results are really incredible!!!. We can learn to play close to optimal solution, with just---and only just---some historical data. Without any knowledge how other agents play, what is their ordering policy, etc. 
					Nowadays that any company has huge datasets, we can easily use those data and optimize their supply chain network. This approach can be easily used in different supply chain with much more complex networks, and more agents. 
					I will post more results latter. BTW, you also can read the corresponding paper <a href="https://arxiv.org/abs/1708.05924">here </a>.!
				<!-- <iframe width="420" height="315" src="https://www.youtube.com/embed/XGSy3_Czz8k"> </iframe> -->				
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<footer>
    		<ul>
        		<li><a href="mailto:oroojlooy@gmail.com">email</a></li>
        		<!-- <li><a href="https://github.com/oroojlooy">github.com/oroojlooy</a></li> -->
        		<li><a href="https://scholar.google.com/citations?user=7nEdMAMAAAAJ&hl=en">Google Scholar</a></li>
				<li><a href="https://www.linkedin.com/in/afshin-oroojlooy-81663b38/">LinkedIn</a></li>
			</ul>
		</footer>
	</body>
</html>
